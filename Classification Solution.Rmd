---
title: "PROG8430-Assignment 4"
author: "Mohammed Saifullah"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Setting the working directory
knitr::opts_knit$set(root.dir = 'E:/Big Data Solution Architecture/PROG8430 - Data Analysis Mathematics, Algorithms and Modeling/Assignment 4')
```


```{r cleanup, include=FALSE}
#Clearing all the Plots, Console and workspace and setting overall number format
if(!is.null(dev.list())) dev.off()
cat("\014") 
rm(list=ls())
options(scipen=9)
```
Loading necessary packages

```{r}
#Load packages
if(!require(tinytex)){install.packages("tinytex")}
library("tinytex")

if(!require(pastecs)){install.packages("pastecs")}
library("pastecs")

if(!require(lattice)){install.packages("lattice")}
library("lattice")

if(!require(vcd)){install.packages("vcd")}
library("vcd")

if(!require(HSAUR)){install.packages("HSAUR")}
library("HSAUR")

if(!require(rmarkdown)){install.packages("rmarkdown")}
library("rmarkdown")

if(!require(ggplot2)){install.packages("ggplot2")}
library("ggplot2")

if(!require(polycor)){install.packages("polycor")}
library("polycor")


if(!require(klaR)){install.packages("klaR")}
library("klaR")

if(!require(MASS)){install.packages("MASS")}
library("MASS")

if(!require(partykit)){install.packages("partykit")}
library("partykit")

if(!require(nnet)){install.packages("nnet")}
library("nnet")

if(!require(corrgram)){install.packages("corrgram")}
library("corrgram")

```


# **Part A - Preliminary Data Preparation**

##1.1 Appending initials to all column names  

```{r}
getwd() #verify working directory
#Read the text data file into a Data Frame
MailOrder_MS <- read.table("PROG8430_Assign04_23W.txt", sep=',', header = TRUE)
#concatenating initial 'MS' to all column names
colnames(MailOrder_MS) <- paste(colnames(MailOrder_MS), "MS", sep = "_")
#Display first 5 rows of the dataset just to verify loading and name transformation is  successful
head(MailOrder_MS, 5)
#Transform String as Factor variable
MailOrder_MS <- as.data.frame(unclass(MailOrder_MS), stringsAsFactors = TRUE)
#Checking Data Structure
str(MailOrder_MS)

```
##1.2 Deleting the observation with PG_MS < 0

```{r}
MailOrder_MS <- MailOrder_MS[!MailOrder_MS$PG_MS < 0,]

summary(MailOrder_MS)
```
##1.3 Creating a new variable in the dataset called OT_MS which will have a value of 1 if DL_MS â‰¤ 8.5 and 0 otherwise
```{r}
MailOrder_MS$OT_MS <- as.factor(ifelse(MailOrder_MS$DL_MS <= 8.5, 1,0))

#Delete the DL_MS variable
MailOrder_MS <- MailOrder_MS[,-c(1)]
head(MailOrder_MS)

```
## Split the data set into Training and Test set

```{r}
#Choosing sampling rate for training data
sr_ms <- 0.8 #80% in training set

# Finding the number of rows of data
n.row <- nrow(MailOrder_MS) #counting number of rows

#Choose the rows for the training sample 

set.seed(6024) #setting a seed, same starting point. Last 4 digits of my student ID
training.rows <- sample(1:n.row, sr_ms*n.row, replace=FALSE) #sampling 
#selecting from 1 to no of rows, how much - sampling-rate*no or rows, placement equal false - don't want to replace

#Assigning to the training sample
train_ms <- subset(MailOrder_MS[training.rows,]) #creating training data set, only keeping training rows

# Assign the balance to the Test Sample

test_ms <- subset(MailOrder_MS[-c(training.rows),]) #keeping everything except training rows

#Checking Train and Test datasets
head(training.rows)
head(train_ms)
head(test_ms)

summary(MailOrder_MS)
summary(test_ms)
summary(train_ms)

```

# **Part 2 - Exploratory Analysis**
##2.1 Checking Correlations
```{r}
#Numeric Correlation
ht_ms <- hetcor(MailOrder_MS) #heterogeneous correlation

round(ht_ms$correlations, 2)

```
**Interpretation:**
There are no significant correlation except PG_MS and OT_MS, which is inversely correlated.

# **Part 3 - Model Development**
##3.1 Full Model
```{r}
full.model_ms <- glm(OT_MS ~ . , data = train_ms, family="binomial", na.action = na.omit)

summary(full.model_ms) 
```
**Interpretation:**
Comparing following measures for full model.  
(1) Fisher iterations - 4, the model converged in reasonable iterations.  
(2) AIC - 399.71  
(3) Residual Deviance - Null daviance of 537.62 and  Residual deviance of 381.71 are not close, so this model is capable of prediction.  
(4) Residual symmetry - Residuals are centered around 0 and symmetric.  
(5) z-values - 5/8 variable have p value less than 0.05, so 5 variable out of 8 passed z-test.  
(6) Parameter Co-Efficients - 7 out of 8 variable have coefficient matching to the correlation matrix.  

##3.2 Back Model

```{r}
back.model_ms = step(full.model_ms, direction="backward", details=TRUE)

summary(back.model_ms)
```

**Interpretation:**
Comparing following measures for backward selection model.  

Comparing following measures for full model.  
(1) Fisher iterations - 5, the model converged in reasonable iterations.  
(2) AIC - 396.07  
(3) Residual Deviance - Null daviance of 537.62 and  Residual deviance of 382.07 are not close, so this model is capable of prediction.  
(4) Residual symmetry - Residuals are centered around 0 and symmetric.  
(5) z-values - 5/6 variable have p value less than 0.05, so 5 variable out of 6 passed z-test.  
(6) Parameter Co-Efficients - 6 out of 6 variable have coefficient matching to the correlation matrix.  


##3.3 Influential Datapoint  
#Full Model Analysis

```{r}
plot(full.model_ms, which=4, id.n=6, main = "Full Model")
r_ms <- residuals(full.model_ms)
head(r_ms)
plot(r_ms, pch=20, main = "Residuals of Full Model")

#Confusion Matrix and measures
resp_glm_ms <- predict(full.model_ms, newdata=train_ms, type="response")   
Class_glm_ms <- ifelse(resp_glm_ms > 0.5,"1","0")           
CF_GLM_MS <- table(train_ms$OT_MS, Class_glm_ms,
                dnn=list("Actual","Predicted") ) 

CF_GLM_MS

TP <- CF_GLM_MS[2,2]
TN <- CF_GLM_MS[1,1]
FP <- CF_GLM_MS[1,2]
FN <- CF_GLM_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_GLM_MS))
sprintf("Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Precision: %.3f", round(Tr_Precision_ms, 3))


```
**Interpretation:**
From the plot we see that all data points are within cook's distance, so there are no significant influential data points.  

#Back Model Analysis

```{r}
plot(back.model_ms, which=4, id.n=6, main = "Back Model")
r_ms <- residuals(back.model_ms)
head(r_ms)
plot(r_ms, pch=20, main = "Residuals of Back Model")

#Confusion Matrix and measures
resp_glm_ms <- predict(back.model_ms, newdata=train_ms, type="response")   
Class_glm_ms <- ifelse(resp_glm_ms > 0.5,"1","0")           
CF_GLM_MS <- table(train_ms$OT_MS, Class_glm_ms,
                dnn=list("Actual","Predicted") ) 

CF_GLM_MS

TP <- CF_GLM_MS[2,2]
TN <- CF_GLM_MS[1,1]
FP <- CF_GLM_MS[1,2]
FN <- CF_GLM_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_GLM_MS))
sprintf("Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Precision: %.3f", round(Tr_Precision_ms, 3))

```
**Interpretation:**
From the plot we see that all data points are within cook's distance, so there are no significant influential data points.  

##3.4 Recommendation 
Based on the above measure I recommend backward model as the superior one.Because :  
- Both model have same accuracy and precision  
- Backward model coefficient matching 6/6  
- 5/6 variable have p value less than 0.05 for backward model as oppose to 5/8 for full model.  

# **Part B **
## 1 Logistic Regression - Stepwise 

```{r}

start_time <- Sys.time()
  
glm.mod_ms = glm(OT_MS ~ .    ,
              family="binomial", data=train_ms, na.action=na.omit)
  
stp_model_ms <- step(glm.mod_ms, trace=FALSE)
  
end_time <- Sys.time()
  
time_ms <- end_time - start_time #model time
  
summary(stp_model_ms)
time_ms
  
# Creating Confusion Matrix on Train Dataset
  
resp_glm_ms <- predict(stp_model_ms, newdata=train_ms, type="response")   
Class_glm_ms <- ifelse(resp_glm_ms > 0.5,"1","0")           
CF_GLM_MS <- table(train_ms$OT_MS, Class_glm_ms,
                dnn=list("Actual","Predicted") ) 

CF_GLM_MS

TP <- CF_GLM_MS[2,2]
TN <- CF_GLM_MS[1,1]
FP <- CF_GLM_MS[1,2]
FN <- CF_GLM_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_GLM_MS))
sprintf("Train Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Train Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Train Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_GLM_MS))
sprintf("Train Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Train Specificity: %.3f", round(Tr_Specificity_ms, 3))


# Creating Confusion Matrix on Test Dataset
  
resp_glm_ms <- predict(stp_model_ms, newdata=test_ms, type="response")   
Class_glm_ms <- ifelse(resp_glm_ms > 0.5,"1","0")           
CF_GLM_MS <- table(test_ms$OT_MS, Class_glm_ms,
                dnn=list("Actual","Predicted") ) 

CF_GLM_MS

TP <- CF_GLM_MS[2,2]
TN <- CF_GLM_MS[1,1]
FP <- CF_GLM_MS[1,2]
FN <- CF_GLM_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_GLM_MS))
sprintf("Test Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Test Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Test Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_GLM_MS))
sprintf("Test Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Test Specificity: %.3f", round(Tr_Specificity_ms, 3))

  
```
**Interpretation:**
Between Train and Test dataset Accuracy, Sensitivity and Specificity are very close. precesion and Prevalence is higher in Test data set.  

## 2 NaÃ¯ve-Bayes Classification

```{r, warning=FALSE}

start_time <- Sys.time()
  
NB.mod_ms <- NaiveBayes(OT_MS ~ . ,
                     data = train_ms, na.action=na.omit)
  
end_time <- Sys.time()
  
time_ms <- end_time - start_time
  

 
time_ms

# Creating Confusion Matrix on Train Dataset

pred_NB_ms <- predict(NB.mod_ms,newdata=train_ms)  
CF_NB_MS <- table(Actual=train_ms$OT_MS, Predicted=pred_NB_ms$class)


CF_NB_MS

TP <- CF_NB_MS[2,2]
TN <- CF_NB_MS[1,1]
FP <- CF_NB_MS[1,2]
FN <- CF_NB_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_NB_MS))
sprintf("Train Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Train Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Train Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_NB_MS))
sprintf("Train Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Train Specificity: %.3f", round(Tr_Specificity_ms, 3))


# Creating Confusion Matrix on Test Dataset
  
pred_NB_ms <- predict(NB.mod_ms,newdata=test_ms)  
CF_NB_MS <- table(Actual=test_ms$OT_MS, Predicted=pred_NB_ms$class)

CF_NB_MS

TP <- CF_NB_MS[2,2]
TN <- CF_NB_MS[1,1]
FP <- CF_NB_MS[1,2]
FN <- CF_NB_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_NB_MS))
sprintf("Test Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Test Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Test Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_NB_MS))
sprintf("Test Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Test Specificity: %.3f", round(Tr_Specificity_ms, 3))



  
```
## 3 Recursive Partitioning Analysis 

```{r}


start_time <- Sys.time()

RP.mod_ms <- ctree(OT_MS ~ ., data=train_ms)

end_time <- Sys.time()
  
time_ms <- end_time - start_time

plot(RP.mod_ms, gp=gpar(fontsize=8))

time_ms

# Creating Confusion Matrix on Train Dataset

pred.RP_ms <- predict(RP.mod_ms, newdata=train_ms)
CF_RP_MS <- table(Actual=train_ms$OT_MS, Predicted=pred.RP_ms)


CF_RP_MS

TP <- CF_RP_MS[2,2]
TN <- CF_RP_MS[1,1]
FP <- CF_RP_MS[1,2]
FN <- CF_RP_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_RP_MS))
sprintf("Train Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Train Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Train Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_RP_MS))
sprintf("Train Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Train Specificity: %.3f", round(Tr_Specificity_ms, 3))


# Creating Confusion Matrix on Test Dataset
  
pred.RP_ms <- predict(RP.mod_ms, newdata=test_ms)
CF_RP_MS <- table(Actual=test_ms$OT_MS, Predicted=pred.RP_ms)

CF_RP_MS

TP <- CF_RP_MS[2,2]
TN <- CF_RP_MS[1,1]
FP <- CF_RP_MS[1,2]
FN <- CF_RP_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_RP_MS))
sprintf("Test Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Test Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Test Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_RP_MS))
sprintf("Test Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Test Specificity: %.3f", round(Tr_Specificity_ms, 3))




```
## 3a Neural Network 

```{r}

start_time <- Sys.time()

set.seed(8430)
nn.mod_ms <- nnet(OT_MS ~ .,
          data=train_ms,
          size=3,
          rang=0.1,
          maxit=1200,
          trace=FALSE)

end_time <- Sys.time()
  
NN_Time_ms <- end_time - start_time

NN_Time_ms


# Creating Confusion Matrix on Train Dataset

pred.nn_ms <- predict(nn.mod_ms, newdata=train_ms, type="class")
CF_NN_MS <- table(Actual=train_ms$OT_MS, Predicted=pred.nn_ms)

CF_NN_MS

TP <- CF_NN_MS[2,2]
TN <- CF_NN_MS[1,1]
FP <- CF_NN_MS[1,2]
FN <- CF_NN_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_NN_MS))
sprintf("Train Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Train Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Train Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_NN_MS))
sprintf("Train Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Train Specificity: %.3f", round(Tr_Specificity_ms, 3))


# Creating Confusion Matrix on Test Dataset
  
pred.nn_ms <- predict(nn.mod_ms, newdata=test_ms, type="class")
CF_NN_MS <- table(Actual=test_ms$OT_MS, Predicted=pred.nn_ms)

CF_NN_MS

TP <- CF_NN_MS[2,2]
TN <- CF_NN_MS[1,1]
FP <- CF_NN_MS[1,2]
FN <- CF_NN_MS[2,1]  

#Calculate Accuracy
Tr_Accuracy_ms <- ((TP+TN)/sum(CF_NN_MS))
sprintf("Test Accuracy: %.3f", round(Tr_Accuracy_ms, 3))
#Calculate Precision
Tr_Precision_ms <- TP/(TP+FP)
sprintf("Test Precision: %.3f", round(Tr_Precision_ms, 3))
#Calculate Sensitivity
Tr_Sensitivity_ms <- TP/(TP+FN)
sprintf("Test Sensitivity: %.3f", round(Tr_Sensitivity_ms, 3))
#Calculate Prevalence
Tr_Prevalence_ms <- (TP+FN)/(sum(CF_NN_MS))
sprintf("Test Prevalence: %.3f", round(Tr_Prevalence_ms, 3))
#Calculate Specificity
Tr_Specificity_ms <- TN/(TN+FP)
sprintf("Test Specificity: %.3f", round(Tr_Specificity_ms, 3))



```
**Interpretation:** 
## 4 Comparing all Classifiers 
Comparing above 4 classifiers based on following measures:

                  |Logistic  | NaÃ¯ve-Bayes | Recursive Partitioning | Neural Network  
 Accuracy         |0.758     | 0.755       | 0.735                  | 0.580  
 Consistency      |3/5       | 4/5         | 1/5                    | 1/5  
 Speed            |0.08980107| 0.04174495  | 0.05557895             | 0.04200816  
 False positive   |48        | 45          | 28                     | 98  

1. Logistic and NaÃ¯ve-Bayes has similar accuracy and tops other 2. 
2. NaÃ¯ve-Bayes more consistent with very close Accuracy, Precision, Sensitivity and Specificity between Train and Test data sets. 
3. NaÃ¯ve-Bayes is most suitable when processing speed is most important. 
4. Recursive Partitioning minimizes false positive most. 

5. Recommendation  
Based on above measures NaÃ¯ve-Bayes classifier model is superior in terms of Accuracy, Consistency and Speed. I recommend NaÃ¯ve-Bayes in this case.  


Final Results, append and write out predictions
```{r, warning=FALSE}

#If the test file is provided read the test file and generate prediction
pred <- predict(NB.mod_ms,newdata=test_ms)

test_final <- cbind(test_ms, pred)
head(test_final)

write.csv(test_final, "Final_Submission_MS.txt")

```

